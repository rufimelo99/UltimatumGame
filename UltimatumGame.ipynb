{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UltimatumGame.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPf5mJcdwaatJ0SSR1Mjbuv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rufimelo99/UltimatumGame/blob/main/UltimatumGame.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3-6wDuzSLsp"
      },
      "source": [
        "#Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_361YcDR8EP"
      },
      "source": [
        "## Player Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdfOGMUVCPxi"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import numpy.random as rnd\n",
        "import math\n",
        "from enum import Enum\n",
        "bargainValues = [0,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85, 0.9,0.95,1.0]\n",
        "mapValuesIndex = {0: 0, 0.05: 1, 0.1: 2, 0.15: 3, 0.2:4, 0.25: 5, 0.3: 6, 0.35: 7, 0.4: 8, 0.45: 9, 0.5: 10, 0.55: 11, 0.6: 12, 0.65: 13, 0.7: 14, 0.75: 15, 0.8: 16, 0.85: 17, 0.9: 18, 0.95: 19, 1: 20}\n",
        "mapIndexValues = {0: 0, 1: 0.05, 2: 0.1, 3: 0.15, 4:0.2, 5: 0.25, 6: 0.3, 7: 0.35, 8: 0.4, 9: 0.45, 10: 5, 11: 5.5, 12: 6, 13: 6.5, 14: 7, 15: 7.5, 16: 8, 17: 8.5, 18: 9, 19: 9.5, 20: 1}\n",
        "\n",
        "class PlayerRole(Enum):\n",
        "    EMPATHIC = 1\n",
        "    PRAGMATIC = 2\n",
        "    INDEPENDENT = 3\n",
        "\n",
        "class Player:\n",
        "    def __init__(self, id, PlayerRole) -> None:\n",
        "        self.id=id\n",
        "        self.neighbours = []\n",
        "        self.ComulativePayoff = 0\n",
        "\n",
        "        self.pValue=0\n",
        "        self.qValue=0\n",
        "        self.PlayerRole=PlayerRole\n",
        "\n",
        "        \n",
        "        self.AvgPayoff=0\n",
        "        self.gamesPlayed=0\n",
        "\n",
        "    def attributeStrategy(self):\n",
        "      if self.PlayerRole == PlayerRole.EMPATHIC :\n",
        "        self.pValue = random.choice(bargainValues)\n",
        "        self.qValue = self.pValue\n",
        "      elif self.PlayerRole == PlayerRole.PRAGMATIC :\n",
        "        self.pValue = random.choice(bargainValues)\n",
        "        self.qValue = 1-self.pValue\n",
        "      elif self.PlayerRole == PlayerRole.INDEPENDENT :\n",
        "        self.pValue = random.choice(bargainValues)\n",
        "        self.qValue = random.choice(bargainValues)\n",
        "\n",
        "\n",
        "\n",
        "    def makeOffer(self):\n",
        "        #returns index of bargain proposal \n",
        "        #offer from Proposer\n",
        "        return mapValuesIndex[self.pValue]\n",
        "\n",
        "    def bargainDecision(self, bargainValueIndex):\n",
        "        if mapIndexValues[bargainValueIndex]>=self.qValue:\n",
        "          return 1\n",
        "        else:\n",
        "          return 0 \n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roC5WK7ISDhd"
      },
      "source": [
        "## Ultimaum Game Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKuFQuV2CRgC"
      },
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "stopsForGraphs = [1,100,1000,10000, 20000]\n",
        "\n",
        "class ultimatumGame:\n",
        "  \n",
        "    def __init__(self, Nplayers, PlayerRole, ScaleFree=False) -> None:\n",
        "        \n",
        "        if ScaleFree:\n",
        "          #Scale-Free\n",
        "          #self.graph = nx.complete_graph(Nplayers)\n",
        "          #ver aqui o argumento n\n",
        "          self.graph = nx.barabasi_albert_graph(Nplayers, 2)\n",
        "        else:\n",
        "          #Erdos-Renyi\n",
        "          self.graph = nx.erdos_renyi_graph(Nplayers, 0.4)\n",
        "\n",
        "        self.Players = {}\n",
        "        \n",
        "        #uniforms distributions i gueess TODO\n",
        "        pValuesForAll = random.choices (bargainValues,k=Nplayers)\n",
        "        #if needed\n",
        "        qValuesForAll = random.choices (bargainValues,k=Nplayers)\n",
        "\n",
        "        #create all players\n",
        "        for i in range(Nplayers):\n",
        "            player = Player(i, PlayerRole)\n",
        "            player.neighbours = list(self.graph.adj[i])\n",
        "\n",
        "            if PlayerRole == PlayerRole.EMPATHIC :\n",
        "              player.pValue = pValuesForAll[i]\n",
        "              player.qValue = player.pValue\n",
        "            elif PlayerRole == PlayerRole.PRAGMATIC :\n",
        "              player.pValue = pValuesForAll[i]\n",
        "              player.qValue = 1-player.pValue\n",
        "            elif PlayerRole == PlayerRole.INDEPENDENT :\n",
        "              player.pValue = pValuesForAll[i]\n",
        "              player.qValue = qValuesForAll[i]\n",
        "\n",
        "\n",
        "            self.Players[i]=player\n",
        "        print(\"Network created\")  \n",
        "\n",
        "    def printGraph(self):\n",
        "        #nx.draw_shell(self.graph, with_labels = True)\n",
        "        #nx.draw(self.graph, with_labels = True)\n",
        "        \n",
        "        fig = plt.figure(figsize=(40, 40)) \n",
        "        nx.draw(self.graph, node_size=150, with_labels = True) \n",
        "        plt.axis('equal') \n",
        "        plt.show() \n",
        "        plt.show()\n",
        "\n",
        "    def runEpisode(self, actualIteration,offersDicInitial,offerDic,thresholdDicInitial,thresholdDic,stopsForGraphs,NaturalSelection=True,SocialPenalty=True):\n",
        "      for playerId in range(len(self.Players)):\n",
        "        if NaturalSelection:\n",
        "          #natural selection -> see if there is a better policy in a random j neighbour and define a probability to use j's strategy \n",
        "          player = self.Players[playerId]\n",
        "          if player.neighbours!=[]:\n",
        "            #see if my random neighbour has a better startegy\n",
        "            randomNeighbourIndex = rnd.choice(len(player.neighbours))\n",
        "            randomNeighbour = self.Players[player.neighbours[randomNeighbourIndex]]\n",
        "            if randomNeighbour.ComulativePayoff > player.ComulativePayoff and actualIteration != 1:\n",
        "              #see here\n",
        "              #usar max\n",
        "              higherDegree=np.max([len(player.neighbours),len(randomNeighbour.neighbours)])\n",
        "\n",
        "              probUsingNeighbourStategy = round(((randomNeighbour.ComulativePayoff-player.ComulativePayoff)/ (2*higherDegree)),3)\n",
        "              prob = probUsingNeighbourStategy\n",
        "              usingExternalPolicy = rnd.choice([0, 1], p = [1-prob, prob])\n",
        "              if usingExternalPolicy:\n",
        "                player.qValue = randomNeighbour.qValue\n",
        "                player.pValue = randomNeighbour.pValue\n",
        "\n",
        "        #episode itself   -> play the game  \n",
        "        for neighbourId in self.Players[playerId].neighbours:\n",
        "          player = self.Players[playerId]\n",
        "          neighbour = self.Players[neighbourId]\n",
        "          \n",
        "          playerBargainIndex=player.makeOffer()\n",
        "          rewardAcceptedOrNot = neighbour.bargainDecision(playerBargainIndex)\n",
        "          \n",
        "          playerReward = bargainValues[-(playerBargainIndex+1)]\n",
        "          neighbourReward = bargainValues[playerBargainIndex]\n",
        "          \n",
        "          #add payoffs to player  \n",
        "          player.ComulativePayoff+=rewardAcceptedOrNot*playerReward\n",
        "          neighbour.ComulativePayoff+=rewardAcceptedOrNot*neighbourReward\n",
        "          \n",
        "          player.gamesPlayed += 1\n",
        "          neighbour.gamesPlayed += 1\n",
        "\n",
        "          #save info for graphics\n",
        "          if actualIteration in stopsForGraphs:\n",
        "            offersDicInitial[bargainValues[playerBargainIndex]]+=1\n",
        "            thresholdDicInitial[bargainValues[-(playerBargainIndex+1)]]+=1\n",
        "      \n",
        "\n",
        "      #save data for graphs\n",
        "      if actualIteration in stopsForGraphs:\n",
        "        offerDic[actualIteration]=offersDicInitial.copy()\n",
        "        offersDicInitial.update({}.fromkeys(offersDicInitial,0))\n",
        "        thresholdDic[actualIteration]=thresholdDicInitial.copy()\n",
        "        thresholdDicInitial.update({}.fromkeys(thresholdDicInitial,0))\n",
        "      \n",
        "\n",
        "      #get the comulative payoffs after a round-robin round\n",
        "      lowerPayoff = len(self.Players) #just a awful large number\n",
        "      lowerPayoffId = 0\n",
        "      for playerId in range(len(self.Players)):\n",
        "        player = self.Players[playerId]\n",
        "        if player.neighbours!=[]:\n",
        "          \n",
        "          player.AvgPayoff = player.ComulativePayoff/player.gamesPlayed\n",
        "          #reseting payoffs of that round\n",
        "          player.ComulativePayoff = 0\n",
        "          player.gamesPlayed = 0\n",
        "          #updating lowerPayoff if necessary\n",
        "          if player.AvgPayoff<lowerPayoff:\n",
        "            lowerPayoff=player.AvgPayoff\n",
        "            lowerPayoffId=player.id\n",
        "            \n",
        "        \n",
        "      #Social penalty\n",
        "      #see neigbours policy->comunidaades\n",
        "      if SocialPenalty:\n",
        "        playerToBeRemoved = self.Players[lowerPayoffId]\n",
        "        #handle its neighbours\n",
        "        for i in playerToBeRemoved.neighbours:\n",
        "          #update its q and p values     \n",
        "          self.Players[i].attributeStrategy()\n",
        "\n",
        "        #update q and p Values\n",
        "        playerToBeRemoved.attributeStrategy()\n",
        "\n",
        "\n",
        "      return offersDicInitial,offerDic,thresholdDicInitial,thresholdDic\n",
        "          "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkFDGZVjSPdD"
      },
      "source": [
        "# Execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQjjDkm8VGiV"
      },
      "source": [
        "## Natural Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn4k1_H_TiwI"
      },
      "source": [
        "### Emphatic Erdos-Renyi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "JMrIWUSpLVeC",
        "outputId": "e70dde17-54ea-4b56-ba68-117b51e24f49"
      },
      "source": [
        "NPLAYERS =  5000\n",
        "NITERATIONS = 10001\n",
        "\n",
        "game = ultimatumGame(NPLAYERS, PlayerRole.EMPATHIC, ScaleFree=False)\n",
        "\n",
        "#pValues\n",
        "offersDicInitial = mapValuesIndex.copy()\n",
        "#qValues\n",
        "thresholdDicInitial = mapValuesIndex.copy()\n",
        "\n",
        "offerDic = {}\n",
        "thresholdDic = {}\n",
        "\n",
        "#run episodes\n",
        "for iteration in range(NITERATIONS):\n",
        "  if iteration%(NITERATIONS//20)==0:\n",
        "    print(\"iteration: \"+str(iteration))\n",
        "  offersDicInitial,offerDic,thresholdDicInitial,thresholdDic = game.runEpisode(iteration,offersDicInitial,offerDic,thresholdDicInitial,thresholdDic,stopsForGraphs, NaturalSelection=True, SocialPenalty=False)\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(bargainValues, np.array(list(offerDic[1].values()))/sum(np.array(list(offerDic[1].values()))) , label = \"1\" )\n",
        "plt.plot(bargainValues, np.array(list(offerDic[100].values()))/sum(np.array(list(offerDic[100].values()))) , label = \"100\" )\n",
        "plt.plot(bargainValues, np.array(list(offerDic[1000].values()))/sum(np.array(list(offerDic[1000].values()))) , label = \"1000\" )\n",
        "plt.plot(bargainValues, np.array(list(offerDic[10000].values()))/sum(np.array(list(offerDic[10000].values()))) , label = \"10000\" )\n",
        "\n",
        "# naming the x axis\n",
        "plt.xlabel('p')\n",
        "# naming the y axis\n",
        "plt.ylabel('d(p)')\n",
        "# show a legend on the plot\n",
        "plt.legend()\n",
        "plt.ylim(bottom=0)\n",
        "plt.ylim(top=1)\n",
        "plt.show()\n",
        "\n",
        "plt.plot(bargainValues, np.array(list(thresholdDic[1].values()))/sum(np.array(list(thresholdDic[1].values()))) , label = \"1\" )\n",
        "plt.plot(bargainValues, np.array(list(thresholdDic[100].values()))/sum(np.array(list(thresholdDic[100].values()))) , label = \"100\" )\n",
        "plt.plot(bargainValues, np.array(list(thresholdDic[1000].values()))/sum(np.array(list(thresholdDic[1000].values()))) , label = \"1000\" )\n",
        "plt.plot(bargainValues, np.array(list(thresholdDic[10000].values()))/sum(np.array(list(thresholdDic[10000].values()))) , label = \"10000\" )\n",
        "\n",
        "# naming the x axis\n",
        "plt.xlabel('q')\n",
        "# naming the y axis\n",
        "plt.ylabel('d(q)')\n",
        "# show a legend on the plot\n",
        "plt.legend()\n",
        "plt.ylim(bottom=0)\n",
        "plt.ylim(top=1)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network created\n",
            "iteration: 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-229accca4e7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNITERATIONS\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iteration: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0moffersDicInitial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mofferDic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthresholdDicInitial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthresholdDic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunEpisode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moffersDicInitial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mofferDic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthresholdDicInitial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthresholdDic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstopsForGraphs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNaturalSelection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocialPenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-fd4c95ffc7b6>\u001b[0m in \u001b[0;36mrunEpisode\u001b[0;34m(self, actualIteration, offersDicInitial, offerDic, thresholdDicInitial, thresholdDic, stopsForGraphs, NaturalSelection, SocialPenalty)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m           \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamesPlayed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m           \u001b[0mneighbour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamesPlayed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m           \u001b[0;31m#save info for graphics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsjzkW6NIp3w"
      },
      "source": [
        "### Emphatic Scale Free"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5BEStkMIp34"
      },
      "source": [
        "NPLAYERS =  5000\n",
        "NITERATIONS = 10001\n",
        "\n",
        "game = ultimatumGame(NPLAYERS, PlayerRole.EMPATHIC, ScaleFree=True)\n",
        "\n",
        "#pValues\n",
        "offersDicInitial = mapValuesIndex.copy()\n",
        "#qValues\n",
        "thresholdDicInitial = mapValuesIndex.copy()\n",
        "\n",
        "offerDic = {}\n",
        "thresholdDic = {}\n",
        "\n",
        "#run episodes\n",
        "for iteration in range(NITERATIONS):\n",
        "  if iteration%(NITERATIONS//20)==0:\n",
        "    print(\"iteration: \"+str(iteration))\n",
        "  offersDicInitial,offerDic,thresholdDicInitial,thresholdDic = game.runEpisode(iteration,offersDicInitial,offerDic,thresholdDicInitial,thresholdDic,stopsForGraphs, NaturalSelection=True, SocialPenalty=False)\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(bargainValues, np.array(list(offerDic[1].values()))/sum(np.array(list(offerDic[1].values()))) , label = \"1\" )\n",
        "plt.plot(bargainValues, np.array(list(offerDic[100].values()))/sum(np.array(list(offerDic[100].values()))) , label = \"100\" )\n",
        "plt.plot(bargainValues, np.array(list(offerDic[1000].values()))/sum(np.array(list(offerDic[1000].values()))) , label = \"1000\" )\n",
        "plt.plot(bargainValues, np.array(list(offerDic[10000].values()))/sum(np.array(list(offerDic[10000].values()))) , label = \"10000\" )\n",
        "\n",
        "# naming the x axis\n",
        "plt.xlabel('p')\n",
        "# naming the y axis\n",
        "plt.ylabel('d(p)')\n",
        "# show a legend on the plot\n",
        "plt.legend()\n",
        "plt.ylim(bottom=0)\n",
        "plt.ylim(top=1)\n",
        "plt.show()\n",
        "\n",
        "plt.plot(bargainValues, np.array(list(thresholdDic[1].values()))/sum(np.array(list(thresholdDic[1].values()))) , label = \"1\" )\n",
        "plt.plot(bargainValues, np.array(list(thresholdDic[100].values()))/sum(np.array(list(thresholdDic[100].values()))) , label = \"100\" )\n",
        "plt.plot(bargainValues, np.array(list(thresholdDic[1000].values()))/sum(np.array(list(thresholdDic[1000].values()))) , label = \"1000\" )\n",
        "plt.plot(bargainValues, np.array(list(thresholdDic[10000].values()))/sum(np.array(list(thresholdDic[10000].values()))) , label = \"10000\" )\n",
        "\n",
        "# naming the x axis\n",
        "plt.xlabel('q')\n",
        "# naming the y axis\n",
        "plt.ylabel('d(q)')\n",
        "# show a legend on the plot\n",
        "plt.legend()\n",
        "plt.ylim(bottom=0)\n",
        "plt.ylim(top=1)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWUFRou4I6iz"
      },
      "source": [
        "### Pragmatic Erdos-Renyi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7iEXJRCI6iz"
      },
      "source": [
        "NPLAYERS =  5000\n",
        "NITERATIONS = 10001\n",
        "\n",
        "game = ultimatumGame(NPLAYERS, PlayerRole.PRAGMATIC, ScaleFree=False)\n",
        "\n",
        "#pValues\n",
        "offersDicInitial = mapValuesIndex.copy()\n",
        "#qValues\n",
        "thresholdDicInitial = mapValuesIndex.copy()\n",
        "\n",
        "offerDic = {}\n",
        "thresholdDic = {}\n",
        "\n",
        "#run episodes\n",
        "for iteration in range(NITERATIONS):\n",
        "  if iteration%(NITERATIONS//20)==0:\n",
        "    print(\"iteration: \"+str(iteration))\n",
        "  offersDicInitial,offerDic,thresholdDicInitial,thresholdDic = game.runEpisode(iteration,offersDicInitial,offerDic,thresholdDicInitial,thresholdDic,stopsForGraphs, NaturalSelection=True, SocialPenalty=False)\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(bargainValues, np.array(list(offerDic[1].values()))/sum(np.array(list(offerDic[1].values()))) , label = \"1\" )\n",
        "plt.plot(bargainValues, np.array(list(offerDic[100].values()))/sum(np.array(list(offerDic[100].values()))) , label = \"100\" )\n",
        "plt.plot(bargainValues, np.array(list(offerDic[1000].values()))/sum(np.array(list(offerDic[1000].values()))) , label = \"1000\" )\n",
        "plt.plot(bargainValues, np.array(list(offerDic[10000].values()))/sum(np.array(list(offerDic[10000].values()))) , label = \"10000\" )\n",
        "\n",
        "# naming the x axis\n",
        "plt.xlabel('p')\n",
        "# naming the y axis\n",
        "plt.ylabel('d(p)')\n",
        "# show a legend on the plot\n",
        "plt.legend()\n",
        "plt.ylim(bottom=0)\n",
        "plt.ylim(top=1)\n",
        "plt.show()\n",
        "\n",
        "plt.plot(bargainValues, np.array(list(thresholdDic[1].values()))/sum(np.array(list(thresholdDic[1].values()))) , label = \"1\" )\n",
        "plt.plot(bargainValues, np.array(list(thresholdDic[100].values()))/sum(np.array(list(thresholdDic[100].values()))) , label = \"100\" )\n",
        "plt.plot(bargainValues, np.array(list(thresholdDic[1000].values()))/sum(np.array(list(thresholdDic[1000].values()))) , label = \"1000\" )\n",
        "plt.plot(bargainValues, np.array(list(thresholdDic[10000].values()))/sum(np.array(list(thresholdDic[10000].values()))) , label = \"10000\" )\n",
        "\n",
        "# naming the x axis\n",
        "plt.xlabel('q')\n",
        "# naming the y axis\n",
        "plt.ylabel('d(q)')\n",
        "# show a legend on the plot\n",
        "plt.legend()\n",
        "plt.ylim(bottom=0)\n",
        "plt.ylim(top=1)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkOeeVnPI1Tb"
      },
      "source": [
        "### Pragmatic Scale Free"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6av4M2UI1Tb"
      },
      "source": [
        "NPLAYERS =  5000\n",
        "NITERATIONS = 10001\n",
        "game = ultimatumGame(NPLAYERS, PlayerRole.PRAGMATIC, ScaleFree=True)\n",
        "\n",
        "#pValues\n",
        "offersDicInitial = mapValuesIndex.copy()\n",
        "#qValues\n",
        "thresholdDicInitial = mapValuesIndex.copy()\n",
        "\n",
        "offerDic = {}\n",
        "thresholdDic = {}\n",
        "\n",
        "#run episodes\n",
        "for iteration in range(NITERATIONS):\n",
        "  if iteration%(NITERATIONS//20)==0:\n",
        "    print(\"iteration: \"+str(iteration))\n",
        "  offersDicInitial,offerDic,thresholdDicInitial,thresholdDic = game.runEpisode(iteration,offersDicInitial,offerDic,thresholdDicInitial,thresholdDic,stopsForGraphs, NaturalSelection=True, SocialPenalty=False)\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(bargainValues, np.array(list(offerDic[1].values()))/sum(np.array(list(offerDic[1].values()))) , label = \"1\" )\n",
        "plt.plot(bargainValues, np.array(list(offerDic[100].values()))/sum(np.array(list(offerDic[100].values()))) , label = \"100\" )\n",
        "plt.plot(bargainValues, np.array(list(offerDic[1000].values()))/sum(np.array(list(offerDic[1000].values()))) , label = \"1000\" )\n",
        "plt.plot(bargainValues, np.array(list(offerDic[10000].values()))/sum(np.array(list(offerDic[10000].values()))) , label = \"10000\" )\n",
        "\n",
        "# naming the x axis\n",
        "plt.xlabel('p')\n",
        "# naming the y axis\n",
        "plt.ylabel('d(p)')\n",
        "# show a legend on the plot\n",
        "plt.legend()\n",
        "plt.ylim(bottom=0)\n",
        "plt.ylim(top=1)\n",
        "plt.show()\n",
        "\n",
        "plt.plot(bargainValues, np.array(list(thresholdDic[1].values()))/sum(np.array(list(thresholdDic[1].values()))) , label = \"1\" )\n",
        "plt.plot(bargainValues, np.array(list(thresholdDic[100].values()))/sum(np.array(list(thresholdDic[100].values()))) , label = \"100\" )\n",
        "plt.plot(bargainValues, np.array(list(thresholdDic[1000].values()))/sum(np.array(list(thresholdDic[1000].values()))) , label = \"1000\" )\n",
        "plt.plot(bargainValues, np.array(list(thresholdDic[10000].values()))/sum(np.array(list(thresholdDic[10000].values()))) , label = \"10000\" )\n",
        "\n",
        "# naming the x axis\n",
        "plt.xlabel('q')\n",
        "# naming the y axis\n",
        "plt.ylabel('d(q)')\n",
        "# show a legend on the plot\n",
        "plt.legend()\n",
        "plt.ylim(bottom=0)\n",
        "plt.ylim(top=1)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOScjMe9M-bm"
      },
      "source": [
        "### Independent Erdos-Renyi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYrv9WhOM-bm"
      },
      "source": [
        "NPLAYERS =  5000\n",
        "NITERATIONS = 10001\n",
        "\n",
        "game = ultimatumGame(NPLAYERS, PlayerRole.INDEPENDENT, ScaleFree=False)\n",
        "\n",
        "#pValues\n",
        "offersDicInitial = mapValuesIndex.copy()\n",
        "#qValues\n",
        "thresholdDicInitial = mapValuesIndex.copy()\n",
        "\n",
        "offerDic = {}\n",
        "thresholdDic = {}\n",
        "\n",
        "#run episodes\n",
        "for iteration in range(NITERATIONS):\n",
        "  if iteration%(NITERATIONS//20)==0:\n",
        "    print(\"iteration: \"+str(iteration))\n",
        "  offersDicInitial,offerDic,thresholdDicInitial,thresholdDic = game.runEpisode(iteration,offersDicInitial,offerDic,thresholdDicInitial,thresholdDic,stopsForGraphs, NaturalSelection=True, SocialPenalty=False)\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(bargainValues, np.array(list(offerDic[1].values()))/sum(np.array(list(offerDic[1].values()))) , label = \"1\" )\n",
        "plt.plot(bargainValues, np.array(list(offerDic[100].values()))/sum(np.array(list(offerDic[100].values()))) , label = \"100\" )\n",
        "plt.plot(bargainValues, np.array(list(offerDic[1000].values()))/sum(np.array(list(offerDic[1000].values()))) , label = \"1000\" )\n",
        "plt.plot(bargainValues, np.array(list(offerDic[10000].values()))/sum(np.array(list(offerDic[10000].values()))) , label = \"10000\" )\n",
        "\n",
        "# naming the x axis\n",
        "plt.xlabel('p')\n",
        "# naming the y axis\n",
        "plt.ylabel('d(p)')\n",
        "# show a legend on the plot\n",
        "plt.legend()\n",
        "plt.ylim(bottom=0)\n",
        "plt.ylim(top=1)\n",
        "plt.show()\n",
        "\n",
        "plt.plot(bargainValues, np.array(list(thresholdDic[1].values()))/sum(np.array(list(thresholdDic[1].values()))) , label = \"1\" )\n",
        "plt.plot(bargainValues, np.array(list(thresholdDic[100].values()))/sum(np.array(list(thresholdDic[100].values()))) , label = \"100\" )\n",
        "plt.plot(bargainValues, np.array(list(thresholdDic[1000].values()))/sum(np.array(list(thresholdDic[1000].values()))) , label = \"1000\" )\n",
        "plt.plot(bargainValues, np.array(list(thresholdDic[10000].values()))/sum(np.array(list(thresholdDic[10000].values()))) , label = \"10000\" )\n",
        "\n",
        "# naming the x axis\n",
        "plt.xlabel('q')\n",
        "# naming the y axis\n",
        "plt.ylabel('d(q)')\n",
        "# show a legend on the plot\n",
        "plt.legend()\n",
        "plt.ylim(bottom=0)\n",
        "plt.ylim(top=1)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xxb2zbVYM-bn"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "ax.scatter(x = [game.Players[playerId].pValue for playerId in game.Players], y = [game.Players[playerId].qValue for playerId in game.Players])\n",
        "plt.xlabel(\"p\")\n",
        "plt.ylabel(\"q\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po3clXvWNBl-"
      },
      "source": [
        "### Independent Scale Free"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXskyk6gNBl_"
      },
      "source": [
        "NPLAYERS =  5000\n",
        "NITERATIONS = 10001\n",
        "\n",
        "game = ultimatumGame(NPLAYERS, PlayerRole.INDEPENDENT, ScaleFree=True)\n",
        "\n",
        "#pValues\n",
        "offersDicInitial = mapValuesIndex.copy()\n",
        "#qValues\n",
        "thresholdDicInitial = mapValuesIndex.copy()\n",
        "\n",
        "offerDic = {}\n",
        "thresholdDic = {}\n",
        "\n",
        "#run episodes\n",
        "for iteration in range(NITERATIONS):\n",
        "  if iteration%(NITERATIONS//20)==0:\n",
        "    print(\"iteration: \"+str(iteration))\n",
        "  offersDicInitial,offerDic,thresholdDicInitial,thresholdDic = game.runEpisode(iteration,offersDicInitial,offerDic,thresholdDicInitial,thresholdDic,stopsForGraphs, NaturalSelection=True, SocialPenalty=False)\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(bargainValues, np.array(list(offerDic[1].values()))/sum(np.array(list(offerDic[1].values()))) , label = \"1\" )\n",
        "plt.plot(bargainValues, np.array(list(offerDic[100].values()))/sum(np.array(list(offerDic[100].values()))) , label = \"100\" )\n",
        "plt.plot(bargainValues, np.array(list(offerDic[1000].values()))/sum(np.array(list(offerDic[1000].values()))) , label = \"1000\" )\n",
        "plt.plot(bargainValues, np.array(list(offerDic[10000].values()))/sum(np.array(list(offerDic[10000].values()))) , label = \"10000\" )\n",
        "\n",
        "# naming the x axis\n",
        "plt.xlabel('p')\n",
        "# naming the y axis\n",
        "plt.ylabel('d(p)')\n",
        "# show a legend on the plot\n",
        "plt.legend()\n",
        "plt.ylim(bottom=0)\n",
        "plt.ylim(top=1)\n",
        "plt.show()\n",
        "\n",
        "plt.plot(bargainValues, np.array(list(thresholdDic[1].values()))/sum(np.array(list(thresholdDic[1].values()))) , label = \"1\" )\n",
        "plt.plot(bargainValues, np.array(list(thresholdDic[100].values()))/sum(np.array(list(thresholdDic[100].values()))) , label = \"100\" )\n",
        "plt.plot(bargainValues, np.array(list(thresholdDic[1000].values()))/sum(np.array(list(thresholdDic[1000].values()))) , label = \"1000\" )\n",
        "plt.plot(bargainValues, np.array(list(thresholdDic[10000].values()))/sum(np.array(list(thresholdDic[10000].values()))) , label = \"10000\" )\n",
        "\n",
        "# naming the x axis\n",
        "plt.xlabel('q')\n",
        "# naming the y axis\n",
        "plt.ylabel('d(q)')\n",
        "# show a legend on the plot\n",
        "plt.legend()\n",
        "plt.ylim(bottom=0)\n",
        "plt.ylim(top=1)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjR5B51TNBl_"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "ax.scatter(x = [game.Players[playerId].pValue for playerId in game.Players], y = [game.Players[playerId].qValue for playerId in game.Players])\n",
        "plt.xlabel(\"p\")\n",
        "plt.ylabel(\"q\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RanfFSFL_J-"
      },
      "source": [
        "#Extra"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXR-xhgxv0FX"
      },
      "source": [
        "degree_sequence = sorted([d for n, d in game.graph.degree()], reverse=True)\n",
        "dmax = max(degree_sequence)\n",
        "\n",
        "fig = plt.figure(\"Degree of a random graph\", figsize=(8, 8))\n",
        "axgrid = fig.add_gridspec(5, 4)\n",
        "\n",
        "ax2 = fig.add_subplot(axgrid[3:, 2:])\n",
        "ax2.plot(*np.unique(degree_sequence, return_counts=True))\n",
        "ax2.set_title(\"Degree histogram\")\n",
        "ax2.set_xlabel(\"Degree\")\n",
        "ax2.set_ylabel(\"# of Nodes\")\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}